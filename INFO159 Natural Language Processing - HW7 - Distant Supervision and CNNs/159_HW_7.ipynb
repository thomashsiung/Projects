{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "159 - HW_7.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KxDT4OEa6G_Q",
        "MNXmHiNw6jRl",
        "jfvHlfmOLpf2"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgQNh_TKTxh5"
      },
      "source": [
        "import json\n",
        "import gensim\n",
        "import sys\n",
        "import gensim.downloader as api\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import nltk\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxDT4OEa6G_Q"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qDRQRrUzuaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e849567-2348-4da7-fedb-56e33538d4df"
      },
      "source": [
        "!python -m nltk.downloader punkt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8htiw0DFSFCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b126f4d-fa61-4af9-a4a3-888bbbca977a"
      },
      "source": [
        "#Let's install qwikidata, which we will use to qurey the wikidata triples\n",
        "!pip install qwikidata\n",
        "#Let's install pywikibot, which we will use to query wikipedia articles\n",
        "!pip install pywikibot\n",
        "!export PYWIKIBOT_NO_USER_CONFIG=1\n",
        "!pip install wikipedia"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting qwikidata\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/40/4273aaaacd7269f80d8ce475aff7115ab8fce31488ba08f3eaca776d110a/qwikidata-0.4.0-py3-none-any.whl\n",
            "Collecting mypy-extensions\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from qwikidata) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->qwikidata) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->qwikidata) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->qwikidata) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->qwikidata) (1.24.3)\n",
            "Installing collected packages: mypy-extensions, qwikidata\n",
            "Successfully installed mypy-extensions-0.4.3 qwikidata-0.4.0\n",
            "Collecting pywikibot\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/b5/589d4e6bc7b97f3be7997fdf79faab5a9aa25e4cf58fc64a11dc771252b0/pywikibot-6.0.1.tar.gz (493kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 20.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20.1 in /usr/local/lib/python3.7/dist-packages (from pywikibot) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=20.2 in /usr/local/lib/python3.7/dist-packages (from pywikibot) (54.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.1->pywikibot) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.1->pywikibot) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.1->pywikibot) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.1->pywikibot) (1.24.3)\n",
            "Building wheels for collected packages: pywikibot\n",
            "  Building wheel for pywikibot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pywikibot: filename=pywikibot-6.0.1-cp37-none-any.whl size=526659 sha256=3f5a8960ff78bdfb9b0c4688657fd0d94be1fb8be01c58924749bccd88c3986a\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/f3/7d/918ba4b40f109ffffa23d3968a9a810cb5f1eac4e19f6ae7ef\n",
            "Successfully built pywikibot\n",
            "Installing collected packages: pywikibot\n",
            "Successfully installed pywikibot-6.0.1\n",
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp37-none-any.whl size=11686 sha256=0a3bdd103ac6f6ddf9fc44949d53285c7e9967a70c71c926739fea38916a2316\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF1Bh7YZMjVT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d52a65b-7d74-4ac2-974b-97f1c6f9992f"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/dbamman/nlp21/main/HW7/glove.6B.50d.50K.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp21/main/HW7/dev_dataset.csv\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp21/main/HW7/train_dataset.csv\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-17 05:28:08--  https://raw.githubusercontent.com/dbamman/nlp21/main/HW7/glove.6B.50d.50K.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21357789 (20M) [text/plain]\n",
            "Saving to: ‘glove.6B.50d.50K.txt’\n",
            "\n",
            "\rglove.6B.50d.50K.tx   0%[                    ]       0  --.-KB/s               \rglove.6B.50d.50K.tx 100%[===================>]  20.37M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2021-04-17 05:28:08 (290 MB/s) - ‘glove.6B.50d.50K.txt’ saved [21357789/21357789]\n",
            "\n",
            "--2021-04-17 05:28:08--  https://raw.githubusercontent.com/dbamman/nlp21/main/HW7/dev_dataset.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 86511 (84K) [text/plain]\n",
            "Saving to: ‘dev_dataset.csv’\n",
            "\n",
            "dev_dataset.csv     100%[===================>]  84.48K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2021-04-17 05:28:09 (29.0 MB/s) - ‘dev_dataset.csv’ saved [86511/86511]\n",
            "\n",
            "--2021-04-17 05:28:09--  https://raw.githubusercontent.com/dbamman/nlp21/main/HW7/train_dataset.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 231438 (226K) [text/plain]\n",
            "Saving to: ‘train_dataset.csv’\n",
            "\n",
            "train_dataset.csv   100%[===================>] 226.01K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2021-04-17 05:28:09 (45.7 MB/s) - ‘train_dataset.csv’ saved [231438/231438]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C21-6PV4xqQf"
      },
      "source": [
        "user_config = '''\n",
        "mylang = 'en'\n",
        "family = 'wikipedia'\n",
        "username = 'ExampleBot'\n",
        "'''\n",
        "with open(\"./user-config.py\", \"w\") as f:\n",
        "  f.write(user_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNXmHiNw6jRl"
      },
      "source": [
        "# **IMPORTANT**: GPU is not enabled by default\n",
        "\n",
        "You must switch runtime environments if your output of the next block of code has an error saying \"ValueError: Expected a cuda device, but got: cpu\"\n",
        "\n",
        "Go to Runtime > Change runtime type > Hardware accelerator > GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bVEu6IP6j2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd90f2c7-3126-4d35-e0c4-ccbc4fdeaa3a"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on {}\".format(device))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfvHlfmOLpf2"
      },
      "source": [
        "# Deliverable 1: Distant Supervision\n",
        "\n",
        "In this component, we will query wikipedia articles and structured triples of information.  We will align the triples with text via the algorithm in SLP Chapter 17 Figure 17.9.  We will be doing this for a small set of Wikipedia articles and relationship types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlyn7pSv5953"
      },
      "source": [
        "First, let's download the text of the Wikipedia articles we are interested in.  For the purpose of Deliverable 1, this is 50 Alternative Hip Hop artists.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cCco2We-OII",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "928e2871-bc95-401f-dfed-3ef03faf7f5a"
      },
      "source": [
        "\n",
        "import pywikibot\n",
        "import wikipedia\n",
        "from pywikibot import pagegenerators\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import requests\n",
        "site = pywikibot.Site()\n",
        "\n",
        "#This line grabs all the wikipedia articles for a given category\n",
        "cat = pywikibot.Category(site,'Category:Alternative hip hop musicians')\n",
        "#Now, we will use pywikibot to grab all wikipedia pages linked to this category\n",
        "gen = pagegenerators.CategorizedPageGenerator(cat)\n",
        "\n",
        "\n",
        "documents = {}\n",
        "#For this homework, we will be stopping once 10 articles are queried\n",
        "counter = 0\n",
        "\n",
        "#iterates through all the pages\n",
        "for page in tqdm(gen):\n",
        "    #grabs the title as plaintext\n",
        "    title = page.title(with_ns=False)\n",
        "    \n",
        "    #If errors, we skip over the entity \n",
        "    try:\n",
        "      #grabs the wikipedia page text\n",
        "      text = wikipedia.page(title)\n",
        "      documents[title] = text.content\n",
        "      counter += 1 \n",
        "    except:\n",
        "      continue\n",
        "    if counter > 50:\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pywikibot/config2.py:945: UserWarning: \n",
            "Configuration variable \"username\" is defined in your user-config.py\n",
            "but unknown. It can be a misspelled one or a variable that is no\n",
            "longer supported.\n",
            "  UserWarning)\n",
            "58it [00:18,  3.22it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzHkOspjd_sv"
      },
      "source": [
        "Let's look at an example of one of the returned wikipedia pages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NyeGR9heCNg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "9041393c-8211-4904-db5f-1efb2575e57d"
      },
      "source": [
        "documents['Kevin Abstract']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Clifford Ian Simpson (born July 16, 1996), well known by his stage name Kevin Abstract, is an American rapper, singer-songwriter, and director, and is a founding member of Brockhampton. Abstract released his debut album, MTV1987, in 2014, and received attention from a number of major music blogs and magazine publications. His second album, American Boyfriend: A Suburban Love Story, was released in November 2016, and his third, Arizona Baby, was released in April 2019.\\n\\n\\n== Early life ==\\nAbstract was born in Corpus Christi, Texas. Abstract never met his father and was not close with his mother, not even knowing what her profession was. He has stated that his family are \"extremely religious\" Mormons. Abstract began producing music at 11 and ran away from home at 15, staying at a friend\\'s house for a year before moving to Georgia to live with his sister.\\n\\n\\n== Music career ==\\n\\n\\n=== 2014: MTV1987 ===\\n\\nAt the beginning of 2014, Abstract began working on his debut album with his in-house record producer, Romil. On April 1, 2014 he released the record\\'s first single, entitled \"Save\". The second single from the album, \"Drugs\" was released on May 6, 2014, with the music video following around a month later. The album was released on July 15, 2014 and was met with positive reviews from a number of music blogs, such as Billboard, Complex, Spin Pigeons & Planes, and 2DopeBoyz.After the release of MTV1987, Abstract began working on the music video for the song \"Hell/Heroina,\" using more than $3,000 accrued from fans through a Kickstarter campaign. The crowdfunded project was directed by Tyler Mitchell and finally released on November 4, 2014. Abstract also did a 24-hour live stream on November 1, leading up to the video release, where viewers watched both himself and his computer screen as the audience listened to him talk, observed him interact with him on Twitter, and watched movies together.  As of November 2018, it has been removed from most major download and streaming services.\\n\\n\\n=== 2015: Death of a Supermodel and NOWIFIII ===\\nOn January 15, 2015, Abstract informed fans via his Twitter page that his second record was to be titled Death of a Supermodel and that it would be released later in the year. However, this album was later discarded. Abstract has announced that he is now working on a different album, They Shoot Horses, a part of the Death of a Supermodel trilogy.\\nOn May 30th, Abstract and other BROCKHAMPTON members, bearface., Romil Hemnani & Henock Sileshi released a side project EP titled \"MEMORIAL DAY\" under new personas and the \"NOWIFIII\" name due to the 2015 Texas floods making them lose their Wi-Fi.\\nOn June 23, Abstract released a video for \"Save\", directed by Ian Gilner, Henock Sileshi, & Franklin Mendez. In November, the first single from They Shoot Horses, \"Echo\" was released, along with a music video directed by Tyler Mitchell.\\n\\n\\n=== 2016–2017: American Boyfriend: A Suburban Love Story and Viceland ===\\n\\nIn March 2016, Abstract joined alternative band The Neighbourhood on a week-long tour through England, opening up shows in Portsmouth, Birmingham, Manchester, and London. The final show of the tour was live streamed via the Periscope app, in which he also teased his new album They Shoot Horses, expected to be released in 2016. He also provided confirmation that his next single would be called Empty.\\nIn April, Abstract announced that he would be going on tour with The Neighbourhood again, but this time in the USA, during May and June. Abstract exclusively performed new songs from his upcoming album.\\nAt the end of June, Abstract announced that he had changed the title of his upcoming album to American Boyfriend: A Suburban Love Story and reassured fans that it would still be coming later that year.\\nAbstract, along with fellow members of Brockhampton, appeared in the Viceland television series American Boyband, which premiered on June 8, 2017. The series followed Abstract on his first headlining tour across North America, while also capturing moments of other Brockhampton members still at home in South Central, Los Angeles.\\nAbstract is currently managed by longtime manager & business partner Anish Ochani and Christian & Kelly Clancy of 4 Strikes.\\n\\n\\n=== Brockhampton ===\\n\\nNear the end of 2014,  AliveSinceForever disbanded. Out of that project, the musical group Brockhampton was formed by Abstract and others he met on the Kanye West fan forum KanyeToThe.In January 2015, Brockhampton\\'s debut single \"Bet I\" was released. The video for the song was released on March 25, 2015, and was directed by fellow Brockhampton members HK Covers and Franklin Mendez.On June 16, Brockhampton released a second single \"Hero\". At the end of June, it was announced that Brockhampton had won VFiles Loud, an online talent contest aiming to discover the best up-and-coming collectives. Brockhampton was given the opportunity to premiere a professionally directed music video for their third single on MTV, as well as having it released through the independent recording label Fool\\'s Gold Records. The single, \"Dirt\", was premiered through Apple\\'s Beats 1 Radio and was released on iTunes a few days after.On March 30, Brockhampton released their first collective mixtape: All-American Trash. On April 22, they also released a documentary showcasing the making of the project.\\nIn 2017, Brockhampton released two albums (Saturation and Saturation II) in the summer of 2017, both to widespread critical acclaim. Three days before the release of Saturation II, Abstract announced via Twitter that a third album titled Saturation III would also be released, marking Brockhampton\\'s third full studio album of the calendar year. Saturation III was released December 15, which also received critical acclaim. Although it was stated by the group that Saturation III would be their final studio album, Brockhampton announced that their fourth studio album, Team Effort, was expected to be released in 2018. However, Team Effort was delayed indefinitely until 2018 following the group\\'s decision to create another album, Puppy, to be released in its place, only for it to be delayed following the sexual assault allegations on fellow Brockhampton member Ameer Vann.On September 21, 2018, Brockhampton released their fourth studio album, Iridescence, recorded at Abbey Road Studios in 10 days. They also released merchandise to promote both the upcoming album and their upcoming tour, named I\\'ll be there. On August 23, 2019, their fifth studio album, Ginger, was released to an overall positive reception.\\n\\n\\n=== 2018: ARIZONA baby ===\\n\\nIn June 2018, Abstract added a picture with the text \"ambfII\" to his Instagram story, with \"ambfII\" presumably standing for American Boyfriend II. Later that year, in October, he tweeted the words \"american boyfriend\", seemingly teasing something. On March 19, 2019, Abstract tweeted a photo, which appears to be the cover of a new album or a single. He also changed his profile picture and his header image. In early April, he expressed in a series of now deleted tweets that once he released as much music as he can in 2019, he would like to retire and focus on filmmaking. Throughout the following weeks, Abstract released more teasers for his upcoming third studio album, ARIZONA baby. With the album to be released in three separate parts, Abstract released a new single titled \"Big Wheels\" on April 9, 2019 with the first part of ARIZONA baby releasing just two days later on April 11, 2019 along with a music video for \"Georgia\". On the same day, Abstract announced the second part to the ARIZONA baby album, Ghettobaby, released April 18, with a music video for \"Baby Boy\". Finally, the full album was released on April 25, alongside a music video for the song \"Peach\".\\n\\n\\n== Influences ==\\nAbstract said his influences for MTV1987 were Frank Ocean, Justin Timberlake, and Kid Cudi. Abstract has described Kanye West, Pink Floyd, Radiohead, Childish Gambino, Outkast, Lana Del Rey and Tyler, the Creator as influences on his music, with Tyler\\'s group, Odd Future, being a large point of reference and inspiration for Brockhampton.\\n\\n\\n== Personal life ==\\nAbstract came out as gay in 2016 and his records often mention his sexuality, saying he will rap about being gay as long as he can imagine a fan in need of a voice.\\n\\n\\n== Solo discography ==\\n\\n\\n=== Studio albums ===\\nMTV1987 (2014)\\nAmerican Boyfriend: A Suburban Love Story (2016)\\nArizona Baby (2019)\\n\\n\\n=== Extended plays ===\\nAllTheGirls (2012)\\nKevin Abstract EP (2013)\\nKilmer (2015)\\nArizona Baby (2019)\\nGhettobaby (2019)\\n\\n\\n=== Mixtapes ===\\nRelax (2009)\\nKevinLovesYou (2009)\\n30 Minutes Til New Years (2009)\\nThe Schoolbus Chronicles (2010)\\nThe Comics (2010)\\nPinkElephants (2011)\\nImagination (2011)\\n\\n\\n=== With Outsiders ===\\nBeyondOurDreams (2011)\\nPeople (2011)\\nInsteadOfDreaming (2012)\\n\\n\\n=== With AliveSinceForever ===\\nForeverMondays! (2011)\\nAliveSinceForever: The Compilation (2012)\\nThe ASF EP (2013)\\n\\n\\n=== With NOWIFIII ===\\nMEMORIAL DAY (2015)\\n\\n\\n=== With Brockhampton ===\\nAll-American Trash (2016)\\nSaturation (2017)\\nSaturation II (2017)\\nSaturation III (2017)\\nIridescence (2018)\\nGinger (2019)\\nRoadrunner: New Light, New Machine (2021)\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nOfficial website'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIpUptt_7wjM"
      },
      "source": [
        "Because the Wikidata corpus is incredibly large, we will use a series of sparql queries to get relevant triples for our corpus.  We will return all relevant triples for alternative hip hop artists and filter out later.\n",
        "\n",
        "Please see https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/Wikidata_Query_Help \n",
        "for more details on Wikidata sparql queries if interested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ejT-QCy4pjG"
      },
      "source": [
        "'''\n",
        "Now we will define the 5 queries we will be using for the distant supervision.\n",
        "\n",
        "We are interested in artists' date of birth, place of birth, school attended,\n",
        "start of musician career, and band name.\n",
        "\n",
        "Please see https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/Wikidata_Query_Help \n",
        "for more information on how to structure queries like this, if interested\n",
        "'''\n",
        "\n",
        "q1 = '''\n",
        "SELECT DISTINCT ?human ?humanLabel ?dob \n",
        "WHERE\n",
        "{\n",
        "    VALUES ?professions {wd:Q177220 wd:Q639669}\n",
        "    ?human wdt:P31 wd:Q5 .\n",
        "    ?human wdt:P106 ?professions .\n",
        "    ?human wdt:P136 ?genre .\n",
        "    ?human wikibase:statements ?statementcount .\n",
        "    ?human wdt:P136 wd:Q438503 .  \n",
        "    ?human wdt:P569 ?dob.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
        "}\n",
        "\n",
        "'''\n",
        "\n",
        "q2 = '''\n",
        "\n",
        "SELECT DISTINCT ?human ?humanLabel ?pobLabel \n",
        "WHERE\n",
        "{\n",
        "    VALUES ?professions {wd:Q177220 wd:Q639669}\n",
        "    ?human wdt:P31 wd:Q5 .\n",
        "    ?human wdt:P106 ?professions .\n",
        "    ?human wdt:P136 ?genre .\n",
        "    ?human wikibase:statements ?statementcount .\n",
        "    ?human wdt:P136 wd:Q438503 .  \n",
        "    ?human wdt:P19 ?pob.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
        "}\n",
        "'''\n",
        "\n",
        "q3 = '''\n",
        "SELECT DISTINCT ?human ?humanLabel ?schoolLabel \n",
        "WHERE\n",
        "{\n",
        "    VALUES ?professions {wd:Q177220 wd:Q639669}\n",
        "    ?human wdt:P31 wd:Q5 .\n",
        "    ?human wdt:P106 ?professions .\n",
        "    ?human wdt:P136 ?genre .\n",
        "    ?human wikibase:statements ?statementcount .\n",
        "    ?human wdt:P136 wd:Q438503 .  \n",
        "    ?human wdt:P69 ?school.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
        "}\n",
        "'''\n",
        "\n",
        "q4 = '''\n",
        "\n",
        "SELECT DISTINCT ?human ?humanLabel ?start \n",
        "WHERE\n",
        "{\n",
        "    VALUES ?professions {wd:Q177220 wd:Q639669}\n",
        "    ?human wdt:P31 wd:Q5 .\n",
        "    ?human wdt:P106 ?professions .\n",
        "    ?human wdt:P136 ?genre .\n",
        "    ?human wikibase:statements ?statementcount .\n",
        "    ?human wdt:P136 wd:Q438503 .  \n",
        "    ?human wdt:P2031 ?start.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
        "}\n",
        "'''\n",
        "\n",
        "q5 = '''\n",
        "SELECT DISTINCT ?human ?humanLabel ?memberLabel\n",
        "WHERE\n",
        "{\n",
        "    VALUES ?professions {wd:Q177220 wd:Q639669}\n",
        "    ?human wdt:P31 wd:Q5 .\n",
        "    ?human wdt:P106 ?professions .\n",
        "    ?human wdt:P136 ?genre .\n",
        "    ?human wikibase:statements ?statementcount .\n",
        "    ?human wdt:P136 wd:Q438503 .  \n",
        "    ?human wdt:P463 ?member.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
        "}\n",
        "\n",
        "'''\n",
        "queries = [q1, q2, q3, q4, q5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCeP63glSFE4"
      },
      "source": [
        "'''\n",
        "Next, let's query wikidata for the triples we are interested in \n",
        "\n",
        "This cell makes a sparql query to wikidata to return triples of info.\n",
        "'''\n",
        "from qwikidata.sparql import (get_subclasses_of_item,\n",
        "                              return_sparql_query_results)\n",
        "from datetime import datetime\n",
        "\n",
        "triples = []\n",
        "query_names = [\"hasDateOfBirth\", \"hasPlaceOfBirth\", \"hasSchool\", \"hasYearStarted\", \"hasMembershipOf\"]\n",
        "count = 0\n",
        "for query in queries:\n",
        "  res = return_sparql_query_results(query)\n",
        "  #We want to save all the triples which are returned from the query\n",
        "  for item in res['results']['bindings']: \n",
        "    if count == 0:\n",
        "      dt = datetime.fromisoformat(item[res['head']['vars'][2]]['value'].split(\"T\")[0])\n",
        "      triples.append((query_names[count], item[res['head']['vars'][1]]['value'], str(dt.strftime(\"%B\")) + \" \" + str(dt.day) + \", \" + str(dt.year)))\n",
        "    elif count == 3:\n",
        "      dt = datetime.fromisoformat(item[res['head']['vars'][2]]['value'].split(\"T\")[0])\n",
        "      triples.append((query_names[count], item[res['head']['vars'][1]]['value'], str(dt.year)))\n",
        "    else:\n",
        "      triples.append((query_names[count], item[res['head']['vars'][1]]['value'], item[res['head']['vars'][2]]['value']))\n",
        "  count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu4nbvOG8ZD1"
      },
      "source": [
        "Let's examine the format of one of these returned triples, indicating that Kelis was born on August 21, 1979."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cBaIbmKABxN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f5be326-c706-4258-c736-51a7ff5d81a1"
      },
      "source": [
        "print(triples[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('hasDateOfBirth', 'Kelis', 'August 21, 1979')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHbIC9V988fB"
      },
      "source": [
        "Now, let's iterate through our Wikipedia articles and align factual triples to sentences from our Wikipedia articles.\n",
        "\n",
        "You will add code to follow the entity alignment algorithm described in Figure 17.9 in SLP3.  Do not worry about the training component of the algorithm; this is covered in Deliverable 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPik_6-_IR4K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7a84115-bafc-4b6b-b8cd-6944a91bdc60"
      },
      "source": [
        "'''\n",
        "Now, we will iterate through and align the triples to sentences to create the dataset.\n",
        "\n",
        "This uses the algorithm from SLP3 Figure 17.9.\n",
        "'''\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "\n",
        "def process_dataset(documents, triples, query_names):\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  aligned_dataset = []\n",
        "\n",
        "  for document in tqdm(documents):\n",
        "    doc = nlp(documents[document])\n",
        "    sentences = list(doc.sents)\n",
        "    for sent in sentences:\n",
        "      sent = sent.text\n",
        "      for relation in query_names:\n",
        "        for triple in [t for t in triples if t[0] == relation]:\n",
        "          should_align = False\n",
        "          \n",
        "          #YOUR CODE HERE\n",
        "          if triple[1] in sent and triple[2] in sent:\n",
        "            should_align = True\n",
        "\n",
        "          if should_align:\n",
        "            #Let's mark the entities with a special prefix and join the multi-word ones with underscores as in SLP\n",
        "            formatted_sent = sent.replace(triple[1], \"_ent1_\" + \"_\".join(triple[1].split(\" \")))\n",
        "            formatted_sent = formatted_sent.replace(triple[2], \"_ent2_\" + \"_\".join(triple[2].split(\" \")))\n",
        "            aligned_dataset.append((formatted_sent, triple[0]))\n",
        "          \n",
        "  return aligned_dataset\n",
        "aligned_dataset = process_dataset(documents, triples, query_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "  2%|▏         | 1/51 [00:00<00:16,  3.04it/s]\u001b[A\n",
            "  4%|▍         | 2/51 [00:00<00:18,  2.69it/s]\u001b[A\n",
            "  6%|▌         | 3/51 [00:01<00:15,  3.02it/s]\u001b[A\n",
            "  8%|▊         | 4/51 [00:01<00:12,  3.66it/s]\u001b[A\n",
            " 12%|█▏        | 6/51 [00:01<00:10,  4.46it/s]\u001b[A\n",
            " 16%|█▌        | 8/51 [00:01<00:09,  4.70it/s]\u001b[A\n",
            " 18%|█▊        | 9/51 [00:01<00:09,  4.57it/s]\u001b[A\n",
            " 22%|██▏       | 11/51 [00:02<00:10,  3.83it/s]\u001b[A\n",
            " 24%|██▎       | 12/51 [00:02<00:08,  4.50it/s]\u001b[A\n",
            " 25%|██▌       | 13/51 [00:03<00:08,  4.49it/s]\u001b[A\n",
            " 27%|██▋       | 14/51 [00:03<00:14,  2.64it/s]\u001b[A\n",
            " 29%|██▉       | 15/51 [00:03<00:10,  3.39it/s]\u001b[A\n",
            " 31%|███▏      | 16/51 [00:04<00:08,  4.11it/s]\u001b[A\n",
            " 33%|███▎      | 17/51 [00:04<00:10,  3.33it/s]\u001b[A\n",
            " 35%|███▌      | 18/51 [00:04<00:09,  3.43it/s]\u001b[A\n",
            " 37%|███▋      | 19/51 [00:05<00:10,  2.99it/s]\u001b[A\n",
            " 39%|███▉      | 20/51 [00:05<00:08,  3.64it/s]\u001b[A\n",
            " 41%|████      | 21/51 [00:05<00:06,  4.31it/s]\u001b[A\n",
            " 43%|████▎     | 22/51 [00:05<00:07,  3.63it/s]\u001b[A\n",
            " 45%|████▌     | 23/51 [00:07<00:18,  1.53it/s]\u001b[A\n",
            " 47%|████▋     | 24/51 [00:08<00:19,  1.36it/s]\u001b[A\n",
            " 51%|█████     | 26/51 [00:08<00:13,  1.85it/s]\u001b[A\n",
            " 53%|█████▎    | 27/51 [00:08<00:10,  2.19it/s]\u001b[A\n",
            " 55%|█████▍    | 28/51 [00:08<00:08,  2.79it/s]\u001b[A\n",
            " 57%|█████▋    | 29/51 [00:08<00:06,  3.41it/s]\u001b[A\n",
            " 59%|█████▉    | 30/51 [00:09<00:05,  3.76it/s]\u001b[A\n",
            " 61%|██████    | 31/51 [00:09<00:05,  3.43it/s]\u001b[A\n",
            " 63%|██████▎   | 32/51 [00:10<00:08,  2.22it/s]\u001b[A\n",
            " 65%|██████▍   | 33/51 [00:10<00:07,  2.34it/s]\u001b[A\n",
            " 69%|██████▊   | 35/51 [00:10<00:05,  2.99it/s]\u001b[A\n",
            " 71%|███████   | 36/51 [00:11<00:04,  3.50it/s]\u001b[A\n",
            " 73%|███████▎  | 37/51 [00:11<00:03,  4.00it/s]\u001b[A\n",
            " 75%|███████▍  | 38/51 [00:11<00:02,  4.64it/s]\u001b[A\n",
            " 76%|███████▋  | 39/51 [00:11<00:02,  5.36it/s]\u001b[A\n",
            " 78%|███████▊  | 40/51 [00:11<00:02,  4.77it/s]\u001b[A\n",
            " 80%|████████  | 41/51 [00:12<00:02,  4.95it/s]\u001b[A\n",
            " 82%|████████▏ | 42/51 [00:12<00:01,  5.63it/s]\u001b[A\n",
            " 84%|████████▍ | 43/51 [00:12<00:01,  4.90it/s]\u001b[A\n",
            " 86%|████████▋ | 44/51 [00:12<00:01,  5.00it/s]\u001b[A\n",
            " 88%|████████▊ | 45/51 [00:12<00:01,  5.64it/s]\u001b[A\n",
            " 90%|█████████ | 46/51 [00:12<00:00,  5.48it/s]\u001b[A\n",
            " 92%|█████████▏| 47/51 [00:14<00:02,  1.70it/s]\u001b[A\n",
            " 94%|█████████▍| 48/51 [00:14<00:01,  1.80it/s]\u001b[A\n",
            " 96%|█████████▌| 49/51 [00:15<00:00,  2.38it/s]\u001b[A\n",
            "100%|██████████| 51/51 [00:15<00:00,  3.34it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJJV56cXAhnn"
      },
      "source": [
        "Now let's look at our newly-aligned dataset, containing a small number of aligned triples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upSGQoJdC5dk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "004c34ca-80dd-4571-ff83-d347003560b8"
      },
      "source": [
        "aligned_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('_ent1_Arabesque, also known as Besque (born Stephen Kawaleet, _ent2_September_17,_1981), is a Juno nominated hip hop artist from Toronto, Ontario, Canada.',\n",
              "  'hasDateOfBirth'),\n",
              " ('_ent1_Arabesque, also known as Besque (born Stephen Kawaleet, September 17, 1981), is a Juno nominated hip hop artist from _ent2_Toronto, Ontario, Canada.',\n",
              "  'hasPlaceOfBirth'),\n",
              " ('Later in the year, he toured with _ent1_Aceyalone from _ent2_Los_Angeles.',\n",
              "  'hasPlaceOfBirth'),\n",
              " ('Daniel Dewan Sewell (born _ent2_March_16,_1981), known professionally as _ent1_Danny_Brown, is an American rapper.',\n",
              "  'hasDateOfBirth'),\n",
              " (\"The Grey Album also got the attention of _ent1_Damon_Albarn, who enlisted Danger Mouse to produce _ent2_Gorillaz' second studio album, Demon Days.\",\n",
              "  'hasMembershipOf'),\n",
              " ('Erik Francis Schrody (born _ent2_August_18,_1969), known by his stage name _ent1_Everlast, is an American musician, singer, rapper, and songwriter, known for his solo work and as the frontman for hip hop group House of Pain.',\n",
              "  'hasDateOfBirth'),\n",
              " ('Erik Francis Schrody (born August 18, 1969), known by his stage name _ent1_Everlast, is an American musician, singer, rapper, and songwriter, known for his solo work and as the frontman for hip hop group _ent2_House_of_Pain.',\n",
              "  'hasMembershipOf'),\n",
              " (\"Following the album's failure, _ent1_Everlast teamed up with fellow Taft High alums DJ Lethal and Danny Boy to form _ent2_House_of_Pain in Los Angeles, California.\",\n",
              "  'hasMembershipOf'),\n",
              " ('early 2006, _ent1_Everlast teamed up with his former House of Pain mates DJ Lethal and Danny Boy to join the hip-hop group _ent2_La_Coka_Nostra.',\n",
              "  'hasMembershipOf'),\n",
              " ('early 2006, _ent1_Everlast teamed up with his former _ent2_House_of_Pain mates DJ Lethal and Danny Boy to join the hip-hop group La Coka Nostra.',\n",
              "  'hasMembershipOf'),\n",
              " (\"On September 7, 2018, _ent1_Everlast's seventh studio album Whitey Ford's _ent2_House_of_Pain was released.\\n\\n\\n\",\n",
              "  'hasMembershipOf'),\n",
              " ('Scott Ramon Seguro Mescudi (born _ent2_January_30,_1984), better known by his stage name _ent1_Kid_Cudi ( KUDD-ee; often stylized as KiD CuDi), is an American rapper, singer, songwriter, record producer, actor, and record executive.',\n",
              "  'hasDateOfBirth'),\n",
              " ('In July _ent2_2008, _ent1_Kid_Cudi released his first mixtape, A Kid Named Cudi (executive produced by Plain Pat and Emile Haynie), in collaboration with New York street-wear brand 10.Deep as a free download.',\n",
              "  'hasYearStarted'),\n",
              " ('_ent1_Kid_Cudi was a prominent songwriter and featured artist on 808s & Heartbreak, with \"Paranoid\" and \"Heartless\" being released as singles, while \"Welcome to Heartbreak\" charted as an album cut and peaked at number 87 on the Pop 100._ent1_Kid_Cudi\\'s first television appearance was at the _ent2_2008 MTV Video Music Awards, alongside Travis Barker and DJ AM.',\n",
              "  'hasYearStarted'),\n",
              " ('_ent1_Kid_Cudi\\'s sound is what inspired and led Kanye West to create his cathartic 808s & Heartbreak (_ent2_2008), with West later stating that he and Cudi were \"the originators of the style, kinda like what Alexander McQueen is to fashion….',\n",
              "  'hasYearStarted')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft8v9zzyAoTm"
      },
      "source": [
        "We've now successfully used distant supervision to align sentences from Wikipedia articles to information triples from Wikidata.  Note that the dataset is not perfect, as it is done without human annotation.  This  process scales up without additional human effort, at the cost of more compute time.  For Deliverable 2, we will be providing a dataset created using Distant Supervision, as the compute-time required to create a sizable dataset is large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv5G5vQAMEk9"
      },
      "source": [
        "# Deliverable 2: Relation Prediction Model\n",
        "\n",
        "Now that we have the process to create an aligned dataset, let's train a CNN-based model to predict a relationship from the text spans.  Note that we will be using a different, larger dataset than the one you created in Deliverable 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loYVNi56BlTg"
      },
      "source": [
        "train_dataset = \"./train_dataset.csv\"\n",
        "dev_dataset = \"./dev_dataset.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qbFLoebIR6d"
      },
      "source": [
        "'''\n",
        "Let's create a dictionary of relation types to define the classification output space\n",
        "For this deliverable we have an additional category, no_relation_found, which can be\n",
        "applied to sentences which do not align with a triple.\n",
        "'''\n",
        "query_names = [\"hasDateOfBirth\", \"hasPlaceOfBirth\", \"hasSchool\", \"hasYearStarted\", \"hasMembershipOf\", \"no_relation_found\"]\n",
        "\n",
        "labels = {}\n",
        "count = 0 \n",
        "for query in query_names:\n",
        "  labels[query] = count\n",
        "  count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9tCtGJOMeMa"
      },
      "source": [
        "'''\n",
        "s1 and s2 define the position embeddings\n",
        "'''\n",
        "def get_batches(x, s1, s2, y, xType, batch_size=12):\n",
        "    batches_x=[]\n",
        "    batches_s1 = []\n",
        "    batches_s2 = []\n",
        "    batches_y=[]\n",
        "    for i in range(0, len(x), batch_size):\n",
        "        #import pdb; pdb.set_trace()\n",
        "        batches_x.append(xType(x[i:i+batch_size]))\n",
        "        batches_s1.append(xType(s1[i:i+batch_size]))\n",
        "        batches_s2.append(xType(s2[i:i+batch_size]))\n",
        "        batches_y.append(torch.LongTensor(y[i:i+batch_size]))\n",
        "    \n",
        "    return batches_x,batches_s1, batches_s2, batches_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0QeNSfb3FdW"
      },
      "source": [
        "PAD_INDEX = 0             # reserved for padding words\n",
        "UNKNOWN_INDEX = 1         # reserved for unknown words\n",
        "SEP_INDEX = 2\n",
        "\n",
        "MAX_DATA_LEN = 300\n",
        "\n",
        "data_lens = []\n",
        "\n",
        "def read_embeddings(filename, vocab_size=50000):\n",
        "  \"\"\"\n",
        "  Utility function, loads in the `vocab_size` most common embeddings from `filename`\n",
        "  \n",
        "  Arguments:\n",
        "  - filename:     path to file\n",
        "                  automatically infers correct embedding dimension from filename\n",
        "  - vocab_size:   maximum number of embeddings to load\n",
        "\n",
        "  Returns \n",
        "  - embeddings:   torch.FloatTensor matrix of size (vocab_size x word_embedding_dim)\n",
        "  - vocab:        dictionary mapping word (str) to index (int) in embedding matrix\n",
        "  \"\"\"\n",
        "\n",
        "  # get the embedding size from the first embedding\n",
        "  with open(filename, encoding=\"utf-8\") as file:\n",
        "    word_embedding_dim = len(file.readline().split(\" \")) - 1\n",
        "\n",
        "  vocab = {}\n",
        "\n",
        "  embeddings = np.zeros((vocab_size, word_embedding_dim))\n",
        "  with open(filename, encoding=\"utf-8\") as file:\n",
        "    for idx, line in enumerate(file):\n",
        "\n",
        "      if idx + 2 >= vocab_size:\n",
        "        break\n",
        "\n",
        "      cols = line.rstrip().split(\" \")\n",
        "      val = np.array(cols[1:])\n",
        "      word = cols[0]\n",
        "      embeddings[idx + 2] = val\n",
        "      vocab[word] = idx + 2\n",
        "  \n",
        "  # a FloatTensor is a multidimensional matrix\n",
        "  # that contains 32-bit floats in every entry\n",
        "  # https://pytorch.org/docs/stable/tensors.html\n",
        "  return torch.FloatTensor(embeddings), vocab\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0g9s_WVKM2l"
      },
      "source": [
        "This format_data() function is where you will add code to determine each word's position from m1 and m2.  As a reminder, we don't want to have negative values in m1_pos_list or m2_pos_list.  To address this, negative values will begin indexing after max_length (300).  For example, the position -10 would be stored as 310, the position -17 would be stored at 317, and so on. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz8m1yjWIR8v"
      },
      "source": [
        "import csv\n",
        "def format_data(filename, vocab, labels, max_length):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      filename: pointer to file holding the dataset we wish to process\n",
        "      vocab: GLoVE vocabulary file created from read_embeddings function\n",
        "      labels: dictionary mapping relationship name to integer index\n",
        "      max_length: maximum length of input\n",
        "    Returns:\n",
        "      data: Input sentences processed as glove embedding indices\n",
        "      data_m1: For each example in the dataset there is a list of positions (one for each word)\n",
        "                from the word to the first entity (appended with _ent1_) with no negative values\n",
        "      data_m2: For each example in the dataset there is a list of positions (one for each word)\n",
        "                from the word to the second entity (appended with _ent2_) with no negative values\n",
        "      data_labels:  Includes the integer label associated with each example in the dataset\n",
        "    \"\"\"    \n",
        "    data = []\n",
        "    data_labels = []\n",
        "    data_m1 = []\n",
        "    data_m2 = []\n",
        "    file = open(filename)\n",
        "    csvreader = csv.reader(file, delimiter=',')\n",
        "\n",
        "    for line in csvreader:\n",
        "        sentence = line[0]\n",
        "        label = line[1]\n",
        "        \n",
        "        m1_pos_list = []\n",
        "        m2_pos_list = []\n",
        "        split_sentence = sentence.split(\" \")\n",
        "\n",
        "        #YOUR CODE HERE\n",
        "        for ind, word in enumerate(split_sentence):\n",
        "          if \"_ent1_\" in word:\n",
        "            m1_ind = ind\n",
        "            break\n",
        "        for ind, word in enumerate(split_sentence):\n",
        "          if \"_ent2_\" in word:\n",
        "            m2_ind = ind\n",
        "            break\n",
        "        \n",
        "        for ind in np.arange(len(split_sentence)):\n",
        "          m1_loc = ind - m1_ind\n",
        "          if m1_loc < 0:\n",
        "            m1_loc = max_length - m1_loc\n",
        "          m2_loc = ind - m2_ind\n",
        "          if m2_loc < 0:\n",
        "            m2_loc = max_length - m2_loc\n",
        "          m1_pos_list.append(m1_loc)\n",
        "          m2_pos_list.append(m2_loc)\n",
        "\n",
        "        w_int = []\n",
        "        for w in nltk.word_tokenize(sentence.lower()):\n",
        "            # skip the unknown words\n",
        "            if w in vocab:\n",
        "                w_int.append(vocab[w])\n",
        "            else:\n",
        "                w_int.append(UNKNOWN_INDEX)\n",
        "        data_lens.append(len(w_int))\n",
        "\n",
        "        #makes sure the example isn't too long for our model\n",
        "        if len(w_int) < 300:\n",
        "          w_int.extend([PAD_INDEX] * (max_length - len(w_int)))\n",
        "          data.append((w_int))\n",
        "          m1_pos_list.extend([max_length-1] * (max_length-len(m1_pos_list)))\n",
        "          data_m1.append(m1_pos_list)\n",
        "          m2_pos_list.extend([max_length*2-1] * (max_length-len(m2_pos_list)))\n",
        "          data_m2.append(m2_pos_list)\n",
        "          data_labels.append(labels[label])\n",
        "    return data, data_m1, data_m2, data_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1raXXcehISDr"
      },
      "source": [
        "class EntityCNNClassifier(nn.Module):\n",
        "\n",
        "   def __init__(self, params, pretrained_embeddings):\n",
        "      super().__init__()\n",
        "      self.seq_len = params[\"max_seq_len\"]\n",
        "      self.num_labels = params[\"label_length\"]\n",
        "      self.embeddings = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
        "\n",
        "      self.m1_embeddings = nn.Embedding(600, 16)\n",
        "      self.m2_embeddings = nn.Embedding(600, 16)\n",
        "\n",
        "      self.conv_2 = nn.Conv1d(82, 16, 2, 1)\n",
        "      self.pool_2 = nn.MaxPool1d(299,1)\n",
        "\n",
        "      self.fc = nn.Linear(16, self.num_labels)\n",
        "    \n",
        "   def forward(self, input, m1_pos_list, m2_pos_list): \n",
        "      x_word_emb = self.embeddings(input)\n",
        "\n",
        "      x_m1 = self.m1_embeddings(m1_pos_list)\n",
        "      x_m2 = self.m1_embeddings(m2_pos_list)\n",
        "\n",
        "      x = torch.cat((x_word_emb, x_m1, x_m2), 2)\n",
        "      x = x.permute(0, 2, 1)\n",
        "    \n",
        "      conv = self.conv_2(x)\n",
        "      conv = torch.tanh(conv)\n",
        "      conv = self.pool_2(conv)\n",
        "      conv = conv.view((conv.shape[0], -1))\n",
        "\n",
        "\n",
        "      self.out = self.fc(conv)\n",
        "      return self.out.squeeze()\n",
        "\n",
        "   def evaluate(self, x, s1, s2, y):\n",
        "      \n",
        "      self.eval()\n",
        "      corr = 0.\n",
        "      total = 0.\n",
        "\n",
        "      with torch.no_grad():\n",
        "\n",
        "        for x, s1, s2, y in zip(x,s1, s2, y):\n",
        "          y_preds=self.forward(x, s1, s2)\n",
        "          for idx, y_pred in enumerate(y_preds):\n",
        "              prediction=torch.argmax(y_pred)\n",
        "              if prediction == y[idx]:\n",
        "                corr += 1.\n",
        "              total+=1                          \n",
        "      return corr/total\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKM9sDbNkQoj"
      },
      "source": [
        "embs, cnn_vocab = read_embeddings(\"glove.6B.50d.50K.txt\")\n",
        "cnn_train_x, cnn_train_s1, cnn_train_s2, cnn_train_y = format_data(train_dataset, cnn_vocab, labels, 300)\n",
        "cnn_dev_x, cnn_dev_s1, cnn_dev_s2, cnn_dev_y = format_data(dev_dataset, cnn_vocab, labels, 300)\n",
        "cnn_trainX, cnn_trainS1, cnn_trainS2, cnn_trainY=get_batches(cnn_train_x, cnn_train_s1, cnn_train_s2, cnn_train_y, torch.LongTensor)\n",
        "cnn_devX, cnn_devS1, cnn_devS2, cnn_devY=get_batches(cnn_dev_x, cnn_dev_s1, cnn_dev_s2, cnn_dev_y, torch.LongTensor)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUzgVW-DTEyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9ece82a-f4d2-406c-f67e-d12f176b56a1"
      },
      "source": [
        "cnnmodel = EntityCNNClassifier(params={\"max_seq_len\": 100, \"label_length\": len(labels)}, pretrained_embeddings=embs)\n",
        "\n",
        "optimizer = torch.optim.Adam(cnnmodel.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "losses = []\n",
        "cross_entropy=nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs=15\n",
        "best_dev_acc = 0.\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    cnnmodel.train()\n",
        "\n",
        "    for x, s1, s2, y in zip(cnn_trainX, cnn_trainS1, cnn_trainS2, cnn_trainY):\n",
        "      y_pred = cnnmodel.forward(x, s1, s2)\n",
        "      loss = cross_entropy(y_pred.view(-1, cnnmodel.num_labels), y.view(-1))\n",
        "      losses.append(loss) \n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    dev_accuracy=cnnmodel.evaluate(cnn_devX, cnn_devS1, cnn_devS2, cnn_devY)\n",
        "    if epoch % 1 == 0:\n",
        "        print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
        "        if dev_accuracy > best_dev_acc:\n",
        "          torch.save(cnnmodel.state_dict(), 'best-cnnmodel-parameters.pt')\n",
        "          best_dev_acc = dev_accuracy\n",
        "\n",
        "cnnmodel.load_state_dict(torch.load('best-cnnmodel-parameters.pt'))\n",
        "print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, dev accuracy: 0.370\n",
            "Epoch 1, dev accuracy: 0.532\n",
            "Epoch 2, dev accuracy: 0.615\n",
            "Epoch 3, dev accuracy: 0.620\n",
            "Epoch 4, dev accuracy: 0.654\n",
            "Epoch 5, dev accuracy: 0.675\n",
            "Epoch 6, dev accuracy: 0.667\n",
            "Epoch 7, dev accuracy: 0.684\n",
            "Epoch 8, dev accuracy: 0.682\n",
            "Epoch 9, dev accuracy: 0.699\n",
            "Epoch 10, dev accuracy: 0.701\n",
            "Epoch 11, dev accuracy: 0.701\n",
            "Epoch 12, dev accuracy: 0.703\n",
            "Epoch 13, dev accuracy: 0.712\n",
            "Epoch 14, dev accuracy: 0.707\n",
            "\n",
            "Best Performing Model achieves dev accuracy of : 0.712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MviKzbYqXqTg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}