# -*- coding: utf-8 -*-
"""SPD_dataset (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iBECFhpimJHnYRKPgFGre2TD2y1WEFPE

## Seattle Police Department Shooting
"""

import numpy as np
import pandas as pd
import datascience as ds
# For visualizations
import matplotlib.pyplot as plt
# For regular expressions
import re
# For handling string
import string
# For performing mathematical operations
import math
import nltk
nltk.download('stopwords')
import seaborn as sns
import wordcloud

# Importing dataset
url = 'https://raw.githubusercontent.com/microsoft-us-ocp-ai/ucbazureworkshop/master/SPD_officer_involved_shooting_data.csv'
spd_df = pd.read_csv(url)
print("Shape of data=>",spd_df.shape)

#this dataset is the csv file of sentiment analysis and text extraction 
url = "https://raw.githubusercontent.com/Stacy-Q/Temporary-data/master/sentiment.csv?token=AF6T56FSPKJL66WAJUI7VGK7MZZDQ"
Sentiment = pd.read_csv(url)
print("Shape of text etr and sentiment analysis data",Sentiment.shape)

spd_df = pd.concat([spd_df, Sentiment], axis=1)

print(spd_df.shape)
spd_df.head()

spd_df1 = spd_df.groupby(['State'])
spd_df1.tail(30)

"""So we can see that there is a single row which where the location is not Seattle but is Sturgis in South Dakota. FOr uniformity puposes, we will be removing this row"""

only_wa = spd_df[(spd_df['State'] == 'WA') & (spd_df['City'] != 'Seattle')]
only_wa.shape

"""Since the dataset is provided by Seattle Police Department and we are focusing on the city of Seattle, we will be dropping the rows with city other than Seattle."""

only_seattle1 = spd_df.drop(spd_df[spd_df['State'] != 'WA'].index, inplace=False)
only_sea = only_seattle1.drop(only_seattle1[only_seattle1['City'] != 'Seattle'].index, inplace=False)
print("Shape of new dataset is ", only_sea.shape )
only_sea.head()

"""now we will be removing the unnecessary columns. This can/will be edited if needed"""

only_sea_col = only_sea[['Longitude', 'Latitude', 'Rank', 'Officer Gender', 'Officer Race', 'Years of SPD Service', 'Officer Injured', 'Number of Rounds','Subject Gender','Subject Age','Subject Race','Subject Weapon','Fatal', 'On-duty', 'Disposition', 'Officer Disciplined?','Summary','Sentiment','Key Phrase']]
print("Shape of the dataframe", only_sea_col.shape)
only_sea_col.head()

#checking for any null values
only_sea_col.isnull().sum()

"""#### should we drop the null values? I think we can drop the officer injured column since we have a limited dataset and there's 11 rows where the values are missing"""

only_sea_col = only_sea_col.drop(['Officer Injured'], axis=1)

only_sea_col.head(3)

only_sea_col[only_sea_col['Disposition']==-1]['Officer Race'].value_counts()

#now converting the categorical values into numbers 
only_sea_col['Officer Gender'].value_counts()

#do we need rank? (included in the EDA)
#check the ranks of officers
#officer,sergeant, detective,lietutenant, outliers
only_sea_col['Rank'].value_counts()

only_sea_col['Subject Weapon'].value_counts()

only_sea_col['Fatal'].value_counts()

only_sea_col['On-duty'].value_counts()

only_sea_col['Disposition'].value_counts()
#throw out missing/try to predict them
#group justified/within policy and unjustified/out of policy
#is there a reason why the disposition is missing?
#should we turn this into numerical values? Justified and within policy are not exactly the same thing and there's too many 
#missing ones

"""#### Note for the categorical values

For the disposition column, we are going to assume that it is reasonable to group Within Policy and Justified together and group Out of Policy and Not Justified together. Ideally, we would have liked to ignore the missing dispositions but since the dataset is limited, we cannot ignore them and they will be labelled as -1
"""

clean_up_dic = {
    'Officer Gender':{'Female':1, 'Male':0},
    'Subject Gender':{'Female':1, "Male":0},
    'Subject Weapon':{'Yes':1, 'No':0},
    'Disposition':{'Within Policy':1, 'Justified':1, 'Out of Policy':0,'Not Justified':0,'Missing':-1},
    'Fatal':{'Yes':1, 'No':0},
    'On-duty':{'Yes':1, 'No':0}    
}

only_sea_col.replace(clean_up_dic, inplace=True)
only_sea_col.head()

"""## Cleaning up the summary column"""

#Taking a look at some of the summaries to see the tone, language and length of the summaries, especially those without a 
#weapon
no_weapon = only_sea_col[only_sea_col['Subject Weapon']==0]

for index,text in enumerate(no_weapon['Summary'][0:5]):
    print('Review %d:\n'%(index+1),text)
    print(" ")
no_weapon.head()

import string
string.punctuation

#next, let's remove the punctuations, and create a new column for that(just to retain the current column until we're sure 
#we don't need it)

def remove_punctuation(text):
    no_punct=[words for words in text if words not in string.punctuation]
    words_wo_punct=''.join(no_punct)
    return words_wo_punct
only_sea_col['summary_wo_punct']=only_sea_col['Summary'].apply(lambda x: remove_punctuation(x))
only_sea_col.head()

#Next step is tokenization which involves splitting the string into a list of words/expressions using Regex

def tokenize(text):
    split=re.split("\W+",text) 
    return split
only_sea_col['summary_wo_punct_split']=only_sea_col['summary_wo_punct'].apply(lambda x: tokenize(x.lower()))
only_sea_col.head()

# Next, we will be removing the stop words, which are the unnecessary words
stopword = nltk.corpus.stopwords.words('english')
print(stopword[:21])

def remove_stopwords(text):
    text=[word for word in text if word not in stopword]
    return text
only_sea_col['summary_wo_punct_split_wo_stopwords'] = only_sea_col['summary_wo_punct_split'].apply(lambda x: remove_stopwords(x))
print(only_sea_col['summary_wo_punct_split_wo_stopwords'][0])

#let's start with gender of the Officer. 
only_sea_col.groupby('Officer Gender')['Fatal'].count()

#next, the race of the officer and the number of fatal encounters
by_race = only_sea_col.groupby(["Officer Race", 'Fatal'])['Fatal'].count()
by_race

#for better visualization
by_race.plot.bar(rot=90)

by_subj_race = only_sea_col.groupby(["Subject Race", 'Fatal'])['Fatal'].count()
by_subj_race.plot.bar(rot=90)

#we will be pivoting the table along the columns Officer Race and Subject Race just to see if we can find any correlation between those 2.
by_subject_race = only_sea_col.groupby(["Officer Race", 'Subject Race','Fatal'])['Fatal'].count()
print(type(by_subject_race))
by_subject_race

by_subject_race_df = pd.DataFrame(by_subject_race)
by_subject_race_df

only_sea_names = only_sea_col.rename(columns={'Subject Age': 'subject_age', 'Subject Gender':'subject_gender'}, inplace=False)
only_sea_names.head()

only_sea_names['subject_age'].value_counts()

#this shows that we need to drop the ages labelled unknown
only_sea_names = only_sea_names[only_sea_names['subject_age'] != 'Unknown']
only_sea_names['subject_age'].value_counts()

#plotting the ages of the subjects 
sns.distplot(only_sea_names.subject_age)
plt.title('Age of individuals shot by police')
plt.show()

"""Thus we can see that while mean is around 30, there are quite a few ages above 100, especially between 110 and 120. I could not find an explanation for this, except that maybe the age was unknown and they decided to allot a very large number.

Wordcloud
"""



#creating wordcloud by race of officer (as an indication of the language, choice of words and frequency of words)
race = "Black"
df_race = only_sea_col[only_sea_col['Officer Race']==race]
#creating a string of all key_phrases
df_race_list = df_race['Key Phrase'].to_list()
df_race_list

df_race['Key Phrase']

final_list = ""
for element in df_race["Key Phrase"]:
  final_list += element
final_list
type(final_list)

#iterating over the elements of the dataframe
only_keywords = df_race[["Key Phrase"]]
keys = ""
for (cName, cData) in only_keywords.iteritems():
   for i in cData:
     keys+= i
keys

# Libraries
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Create a list of word
text= keys
 
# Create the wordcloud object
wordcloud = WordCloud(width=480, height=480, margin=0).generate(text)
 
# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.margins(x=0, y=0)
plt.show()

