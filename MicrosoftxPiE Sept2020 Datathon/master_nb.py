# -*- coding: utf-8 -*-
"""Master_nb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BGLmcmy2bsvp2dIbrFV8rRVbUO2raYQA

# Library, Data, API Configration #
"""

#Library
import numpy as np 
import pandas as pd
# For visualizations
import matplotlib.pyplot as plt
# For regular expressions
import re
# For handling string
import string
# For performing mathematical operations
import math
import nltk
nltk.download('stopwords')

# loading the dataset
url = 'https://raw.githubusercontent.com/microsoft-us-ocp-ai/ucbazureworkshop/master/SPD_officer_involved_shooting_data.csv'
SPD = pd.read_csv(url)
url2 = "https://raw.githubusercontent.com/microsoft-us-ocp-ai/ucbazureworkshop/master/support_ticket_data.csv"
customer = pd.read_csv(url2, encoding ='latin1')

SPD.head()

customer.head()

import os
import requests
from pprint import pprint

# Fill in your credentials
subscription_key = "ed3880d20cb048a2aba7fad3ca9e1201"
endpoint = "https://data-text-analytics.cognitiveservices.azure.com/"

def sentiment_analysis_example(documents):
    sentiment_url = endpoint + "/text/analytics/v3.0/sentiment"
    headers = {"Ocp-Apim-Subscription-Key": subscription_key}
    response = requests.post(sentiment_url, headers=headers, json=documents)
    sentiments = response.json()

    print("Printing sentiments ... \n")
    pprint(sentiments)
    return sentiments


def extract_key_phrases(documents):
    keyphrase_url = endpoint + "/text/analytics/v3.0/keyphrases"
    headers = {"Ocp-Apim-Subscription-Key": subscription_key}
    response = requests.post(keyphrase_url, headers=headers, json=documents)
    key_phrases = response.json()

    print("Printing key phrases ... \n")
    pprint(key_phrases)
    return key_phrases


def identify_entities(documents):
    entities_url = endpoint + "/text/analytics/v3.0/entities/recognition/general"
    headers = {"Ocp-Apim-Subscription-Key": subscription_key}
    response = requests.post(entities_url, headers=headers, json=documents)
    entities = response.json()
    pprint(entities)


def convert_text_to_JSON(data):
    dictlist = [dict(id = i, language = "en", text = data[i]) for i in range(len(data))]
    documents = {"documents": dictlist}
    return documents


def parse_output(output_JSON):
    """
    Convert the response body from the API request to
    select the values you want.
    """
    pass


if __name__ == "__main__":
    """
    Read in your data here and call the functions above.
    You'll likely do a bulk of your coding here.

    - `documents`: an example of the required input format by the Text Analytics API
    """

    documents = {"documents": [
        {"id": "1", "language": "en",
            "text": "I do not like this hammer made by Black & Decker. It does not work correctly. I want to request a return."},
        {"id": "2", "language": "es",
            "text": "I've been trying to talk to someone about my sink problem. It won't hold all of my fish."}
    ]}

    # Uncomment the line below if you choose to use the SDK in the future
    #client = authenticate_client()
    #sentiments = sentiment_analysis_example(documents)
    key_phrases = extract_key_phrases(documents)
    #entities = identify_entities(documents)

spd_df1 = SPD.groupby(['State'])
spd_df1.tail(30)

only_wa = SPD[(SPD['State'] == 'WA') & (SPD['City'] != 'Seattle')]
only_wa.shape

only_seattle1 = SPD.drop(SPD[SPD['State'] != 'WA'].index, inplace=False)
only_sea = only_seattle1.drop(only_seattle1[only_seattle1['City'] != 'Seattle'].index, inplace=False)
print("Shape of new dataset is ", only_sea.shape )
only_sea.head()

only_sea_col = only_sea[['Longitude', 'Latitude', 'Rank', 'Officer Gender', 'Officer Race', 'Years of SPD Service', 'Officer Injured', 'Number of Rounds','Subject Gender','Subject Race','Subject Weapon','Fatal', 'On-duty', 'Disposition', 'Officer Disciplined?','Summary']]
print("Shape of the dataframe", only_sea_col.shape)
only_sea_col.head()

#checking for any null values
only_sea_col.isnull().sum()

only_sea.to_csv('/drive/My Drive/sea.csv')

only_sea_col = only_sea_col.drop(['Officer Injured'], axis=1)

only_sea_col.head(3)

#now converting the categorical values into numbers 
only_sea_col['Officer Gender'].value_counts()

#do we need rank? (included in the EDA)
#check the ranks of officers
#officer,sergeant, detective,lietutenant, outliers
only_sea_col['Rank'].value_counts()

only_sea_col['Subject Weapon'].value_counts()

only_sea_col['Fatal'].value_counts()

only_sea_col['On-duty'].value_counts()

only_sea_col['Disposition'].value_counts()

clean_up_dic = {
    'Officer Gender':{'Female':1, 'Male':0},
    'Subject Gender':{'Female':1, "Male":0},
    'Subject Weapon':{'Yes':1, 'No':0},
    'Fatal':{'Yes':1, 'No':0},
    'On-duty':{'Yes':1, 'No':0}    
}

only_sea_col.replace(clean_up_dic, inplace=True)
only_sea_col.head()

#Taking a look at some of the summaries to see the tone, language and length of the summaries, especially those without a 
#weapon
no_weapon = only_sea_col[only_sea_col['Subject Weapon']==0]

for index,text in enumerate(no_weapon['Summary'][0:5]):
    print('Review %d:\n'%(index+1),text)
    print(" ")
no_weapon.head()

#next, the race of the officer
by_race = only_sea_col.groupby(["Officer Race"]).agg(['count'])
by_race['Fatal']

"""## Utilizing Azure APIs ##"""

api_prep_SPD = pd.DataFrame()
api_prep_SPD.loc[:, "text"] = SPD["Summary"]
api_prep_SPD.loc[:,"language"] = ["en"] * len(SPD)
api_prep_SPD.loc[:, "id"] = np.arange(1, len(SPD) + 1)
api_prep_SPD['id'] = api_prep_SPD['id'].astype('str')
api_prep_SPD

def api_call(text, language, id):  
  document_value = [{"text" : text, "id" : id, "language" : language}]
  SPD_API_DICT = {"documents": document_value}
  SPD_API_DICT
  SPD_key_phrases = extract_key_phrases(SPD_API_DICT)
  return SPD_key_phrases["documents"][0]["keyPhrases"]

#### DO NOT RUN AGAIN !!!!! RUN TIME SO LONG !!! RESULT ALREADY SAVED IN CSV AND LOAD AHEAD.
#api_prep_SPD["Key Phrase"] = api_prep_SPD.apply(lambda x:api_call(x["text"], x["language"], x["id"]) , axis = 1)

# Save the dataframe so we don't need to call api everytime we run the notebook
#from google.colab import drive
#drive.mount('/drive')
#api_prep_SPD.to_csv('/drive/My Drive/key_phrase.csv')

url = "https://raw.githubusercontent.com/Stacy-Q/Temporary-data/master/key_phrase.csv?token=AF6T56ARTULGSSEMZYA7X4K7MZS3Y"
Key_phrade = pd.read_csv(url)
Key_phrade

Key_phrade["Key Phrase"][0]

#api_prep_SPD = api_prep_SPD[0::]
#document_value = api_prep_SPD.to_dict("record")
#SPD_API_DICT = {"documents": document_value}
#SPD_API_DICT
#SPD_sentiment = sentiment_analysis_example(SPD_API_DICT)
#SPD_sentiment

SPD_sentiment["documents"][0]["confidenceScores"]

def api_call_sentiment(text, language, id):  
  document_value = [{"text" : text, "id" : id, "language" : language}]
  SPD_API_DICT = {"documents": document_value}
  SPD_API_DICT
  SPD_sentiment = sentiment_analysis_example(SPD_API_DICT)
  return SPD_sentiment["documents"][0]["confidenceScores"]

api_prep_SPD["Sentiment"] = api_prep_SPD.apply(lambda x:api_call_sentiment(x["text"], x["language"], x["id"]) , axis = 1)

api_prep_SPD

api_prep_SPD.to_csv('/drive/My Drive/sentiment.csv')

url = "https://raw.githubusercontent.com/Stacy-Q/Temporary-data/master/sentiment.csv?token=AF6T56FSPKJL66WAJUI7VGK7MZZDQ"
Sentiment = pd.read_csv(url)
Sentiment

pip install wordcloud

from wordcloud import WordCloud, STOPWORDS, ImageColorGenerato

text = df.description[0]